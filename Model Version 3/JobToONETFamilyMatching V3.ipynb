{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PURPOSE OF THIS NOTEBOOK: \n",
    "\n",
    "This notebook contains the training and testing of the third version of the Job Title to ONET Family Matching model. This time, we are focusing on maintaining model performance while significantly reducing the size of the model. I will be experimenting with different models until I land one that is exactly what I need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "        AutoModelForCausalLM, \n",
    "        AutoTokenizer, \n",
    "        BitsAndBytesConfig, \n",
    "        TrainingArguments,\n",
    "        HfArgumentParser,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        DataCollatorForLanguageModeling,\n",
    "        EarlyStoppingCallback,\n",
    "        pipeline,\n",
    "        logging,\n",
    "        set_seed)\n",
    "from tqdm.autonotebook import tqdm \n",
    "from functools import partial\n",
    "from torch import cuda\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from CommonFunctions import read_and_prepare_data, CustomDataset, Loss\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from random import randrange\n",
    "\n",
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98543ab1158742afbef26c5b262ab7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_tokenizer():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4_bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{6900}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        use_safetensors=True,\n",
    "        quantization_config=bnb_config,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side=\"right\"\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb382395e9245bf862f804322b18961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = create_model_and_tokenizer()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure for Llama model:\n",
    "\n",
    "From the reading I have done about the llama model, this model compared to BERT expects a different format of input.\n",
    "The formatting goes as follows:   \n",
    "   \n",
    "**Instruction**: Prompt for the model, think of telling it what to do. In this case we're asking the model to categorize a job title to one of x ONET families/codes  \n",
    "  \n",
    "**Input**: the job title  \n",
    "  \n",
    "**Output**: The resulting expected ONET code.  \n",
    "  \n",
    "***Now from here I would need to create a new version of the test/train dataframes with a new DataManipulation.ipynb file.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the directory of the output \n",
    "OUTPUT_DIR = \"Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import test and training data\n",
    "train = load_dataset(\"csv\", data_files=\"../Data/Training_Data.csv\")\n",
    "test = load_dataset(\"csv\", data_files=\"../Data/TestingData.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Categorize the job title into one of the 22 job families:\\n\\n11\\n13\\n15\\n17\\n19\\n21\\n23\\n25\\n27\\n29\\n31\\n33\\n35\\n37\\n39\\n41\\n43\\n45\\n47\\n49\\n51\\n53\\n\\n', 'input': 'Security Consultant', 'output': 13}\n"
     ]
    }
   ],
   "source": [
    "print(train['train'][randrange(len(train['train']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt formats \n",
    "def create_prompt(sample):\n",
    "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    INSTRUCTION_KEY = \"### Instruction:\"\n",
    "    INPUT_KEY = \"Input:\"\n",
    "    RESPONSE_KEY = \"### Response:\"\n",
    "    END_KEY = \"### End\"\n",
    "\n",
    "    # create static strings for prompt\n",
    "    intro = f'{INTRO_BLURB}'\n",
    "    instruction = f\"{INSTRUCTION_KEY}\\n{sample['instruction']}\"\n",
    "    input = f\"{INPUT_KEY}\\n{sample['input']}\" if sample['input'] else None\n",
    "    response = f\"{RESPONSE_KEY}\\n{sample['output']}\"\n",
    "    end = f'{END_KEY}'\n",
    "\n",
    "    # turn strings into a list of strings. \n",
    "    parts = [part for part in [intro, instruction, input, response, end] if part]\n",
    "\n",
    "    # join the prompt template into one string \n",
    "    formatted_prompt = \"\\n\\n\".join(parts)\n",
    "\n",
    "    # store formatted prompt into a key \"text\"\n",
    "    sample['text'] = formatted_prompt\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Categorize the job title into one of the 22 job families:\\n\\n11\\n13\\n15\\n17\\n19\\n21\\n23\\n25\\n27\\n29\\n31\\n33\\n35\\n37\\n39\\n41\\n43\\n45\\n47\\n49\\n51\\n53\\n\\n', 'input': 'Spinneret Cleaner', 'output': 53, 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the job title into one of the 22 job families:\\n\\n11\\n13\\n15\\n17\\n19\\n21\\n23\\n25\\n27\\n29\\n31\\n33\\n35\\n37\\n39\\n41\\n43\\n45\\n47\\n49\\n51\\n53\\n\\n\\n\\nInput:\\nSpinneret Cleaner\\n\\n### Response:\\n53\\n\\n### End'}\n"
     ]
    }
   ],
   "source": [
    "print(create_prompt(train['train'][randrange(len(train['train']))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(model):\n",
    "    # Pull model config\n",
    "    conf = model.config\n",
    "\n",
    "    max_length = None\n",
    "\n",
    "    for length_setting in ['n_positions', 'max_position_embeddings', 'seq_length']:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"found max length: {max_length}\")\n",
    "            break\n",
    "        if not max_length:\n",
    "            max_length = 1024\n",
    "            print(f\"using default max length: {max_length}\")\n",
    "        return max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    print(\"preprocessing tokenizer...\")\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        max_length = max_length,\n",
    "        truncation = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(tokenizer: AutoTokenizer, max_length: int, seed, dataset: str):\n",
    "    # Add prompt to each sample\n",
    "    print(\"Preprocessing dataset...\")\n",
    "    dataset = dataset.map(create_prompt)\n",
    "\n",
    "    # Apply preprocessing to each batch of the dataset & and remove \"instruction\", \"input\", \"output\", and \"text\" fields\n",
    "    _preprocessing_function = partial(preprocess_batch, max_length = max_length, tokenizer = tokenizer)\n",
    "    dataset = dataset.map(\n",
    "        _preprocessing_function,\n",
    "        batched = True,\n",
    "        remove_columns = [\"instruction\", \"input\", \"output\", \"text\"],\n",
    "    )\n",
    "\n",
    "    # Filter out samples that have \"input_ids\" exceeding \"max_length\"\n",
    "    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n",
    "\n",
    "    # Shuffle dataset\n",
    "    dataset = dataset.shuffle(seed = seed)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using default max length: 1024\n",
      "Preprocessing dataset...\n",
      "Preprocessing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd9d890c9124bf290f0c11ab9353cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13483 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n",
      "preprocessing tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc0f10e58da4dfeb5f92aad812eee98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/13483 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42\n",
    "max_length = get_max_length(model)\n",
    "preprocessed_train = preprocess_dataset(tokenizer, max_length, seed, dataset=train)\n",
    "preprocessed_test = preprocess_dataset(tokenizer, max_length, seed, dataset=test)\n",
    "OUTPUT_DIR = '../Data/Models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(r, lora_alpha, target_modules, lora_dropout, bias, task_type):\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r = r,\n",
    "        lora_alpha = lora_alpha,\n",
    "        target_modules = target_modules,\n",
    "        lora_dropout = lora_dropout,\n",
    "        bias = bias,\n",
    "        task_type = task_type,\n",
    "    )\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    print(f\"LoRA module names: {list(lora_module_names)}\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model, use_4bit = False):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "\n",
    "    :param model: PEFT model\n",
    "    \"\"\"\n",
    "\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        num_params = param.numel()\n",
    "        if num_params == 0 and hasattr(param, \"ds_numel\"):\n",
    "            num_params = param.ds_numel\n",
    "        all_param += num_params\n",
    "        if param.requires_grad:\n",
    "            trainable_params += num_params\n",
    "\n",
    "    if use_4bit:\n",
    "        trainable_params /= 2\n",
    "\n",
    "    print(\n",
    "        f\"All Parameters: {all_param:,d} || Trainable Parameters: {trainable_params:,d} || Trainable Parameters %: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingarguments = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=1,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    warmup_ratio=0.05,\n",
    "    save_strategy='epoch',\n",
    "    group_by_length=True,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    report_to='tensorboard',\n",
    "    save_safetensors=True,\n",
    "    lr_scheduler_type='cosine',\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_train = preprocessed_train['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA module names: ['down_proj', 'up_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'v_proj']\n",
      "All Parameters: 3,540,389,888 || Trainable Parameters: 39,976,960 || Trainable Parameters %: 1.1291682911958425\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "959fd8f02ac14b3d8b35f65921c7ee0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/omah/Documents/GitHub/ResumeSkillExtraction/resumeenv/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:224: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7048, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
      "{'loss': 1.757, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 1.4412, 'learning_rate': 0.00018888888888888888, 'epoch': 0.0}\n",
      "{'loss': 1.012, 'learning_rate': 0.00017777777777777779, 'epoch': 0.0}\n",
      "{'loss': 0.6915, 'learning_rate': 0.0001666666666666667, 'epoch': 0.0}\n",
      "{'loss': 0.468, 'learning_rate': 0.00015555555555555556, 'epoch': 0.0}\n",
      "{'loss': 0.3466, 'learning_rate': 0.00014444444444444444, 'epoch': 0.0}\n",
      "{'loss': 0.3097, 'learning_rate': 0.00013333333333333334, 'epoch': 0.0}\n",
      "{'loss': 0.2253, 'learning_rate': 0.00012222222222222224, 'epoch': 0.0}\n",
      "{'loss': 0.2133, 'learning_rate': 0.00011111111111111112, 'epoch': 0.0}\n",
      "{'loss': 0.2105, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
      "{'loss': 0.1834, 'learning_rate': 8.888888888888889e-05, 'epoch': 0.0}\n",
      "{'loss': 0.1857, 'learning_rate': 7.777777777777778e-05, 'epoch': 0.0}\n",
      "{'loss': 0.1975, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 0.1774, 'learning_rate': 5.555555555555556e-05, 'epoch': 0.0}\n",
      "{'loss': 0.1555, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.0}\n",
      "{'loss': 0.1656, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.0}\n",
      "{'loss': 0.1486, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.0}\n",
      "{'loss': 0.1788, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.0}\n",
      "{'loss': 0.1308, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'train_runtime': 200.0608, 'train_samples_per_second': 0.4, 'train_steps_per_second': 0.1, 'train_loss': 0.49516828283667563, 'epoch': 0.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        0.0\n",
      "  train_loss               =     0.4952\n",
      "  train_runtime            = 0:03:20.06\n",
      "  train_samples_per_second =        0.4\n",
      "  train_steps_per_second   =        0.1\n",
      "{'train_runtime': 200.0608, 'train_samples_per_second': 0.4, 'train_steps_per_second': 0.1, 'train_loss': 0.49516828283667563, 'epoch': 0.0}\n",
      "Saving last checkpoint of the model...\n"
     ]
    }
   ],
   "source": [
    "fine_tune(model, tokenizer, preprocessed_train, lora_r, lora_alpha, lora_dropout, bias, task_type, per_device_train_batch_size, gradient_accumulation_steps, warmup_steps, max_steps, learning_rate, fp16, logging_steps, output_dir, optim, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadc0290ca0c48988f5e08ac7fb58e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Need either a `state_dict` or a `save_folder` containing offloaded weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# save the model and evaluate in the future.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Load fine-tuned weights\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m AutoPeftModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(output_dir, device_map \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m, torch_dtype \u001b[39m=\u001b[39;49m torch\u001b[39m.\u001b[39;49mbfloat16)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Merge the LoRA layers with the base model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmerge_and_unload()\n",
      "File \u001b[0;32m~/Documents/GitHub/ResumeSkillExtraction/resumeenv/lib/python3.10/site-packages/peft/auto.py:101\u001b[0m, in \u001b[0;36m_BaseAutoPeftModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, adapter_name, is_trainable, config, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot infer the auto class from the config, please make sure that you are loading the correct model for your task type.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[0;32m--> 101\u001b[0m base_model \u001b[39m=\u001b[39m target_class\u001b[39m.\u001b[39;49mfrom_pretrained(base_model_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_target_peft_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    104\u001b[0m     base_model,\n\u001b[1;32m    105\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    110\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/ResumeSkillExtraction/resumeenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:484\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    483\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 484\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    485\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    486\u001b[0m     )\n\u001b[1;32m    487\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    488\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    489\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/GitHub/ResumeSkillExtraction/resumeenv/lib/python3.10/site-packages/transformers/modeling_utils.py:2937\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mskip_keys\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m inspect\u001b[39m.\u001b[39msignature(dispatch_model)\u001b[39m.\u001b[39mparameters:\n\u001b[1;32m   2936\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mskip_keys\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_skip_keys_device_placement\n\u001b[0;32m-> 2937\u001b[0m     dispatch_model(model, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2939\u001b[0m \u001b[39mif\u001b[39;00m output_loading_info:\n\u001b[1;32m   2940\u001b[0m     \u001b[39mif\u001b[39;00m loading_info \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/ResumeSkillExtraction/resumeenv/lib/python3.10/site-packages/accelerate/big_modeling.py:366\u001b[0m, in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m save_folder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m offload_index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    365\u001b[0m     device \u001b[39m=\u001b[39m main_device \u001b[39mif\u001b[39;00m offload_index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m     weights_map \u001b[39m=\u001b[39m OffloadedWeightsLoader(\n\u001b[1;32m    367\u001b[0m         state_dict\u001b[39m=\u001b[39;49mstate_dict, save_folder\u001b[39m=\u001b[39;49msave_folder, index\u001b[39m=\u001b[39;49moffload_index, device\u001b[39m=\u001b[39;49mdevice\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     weights_map \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/ResumeSkillExtraction/resumeenv/lib/python3.10/site-packages/accelerate/utils/offload.py:154\u001b[0m, in \u001b[0;36mOffloadedWeightsLoader.__init__\u001b[0;34m(self, state_dict, save_folder, index, device)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    148\u001b[0m     state_dict: Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     device\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    152\u001b[0m ):\n\u001b[1;32m    153\u001b[0m     \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m save_folder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNeed either a `state_dict` or a `save_folder` containing offloaded weights.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_dict \u001b[39m=\u001b[39m {} \u001b[39mif\u001b[39;00m state_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m state_dict\n\u001b[1;32m    157\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_folder \u001b[39m=\u001b[39m save_folder\n",
      "\u001b[0;31mValueError\u001b[0m: Need either a `state_dict` or a `save_folder` containing offloaded weights."
     ]
    }
   ],
   "source": [
    "# save the model and evaluate in the future.\n",
    "# Load fine-tuned weights\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(output_dir, device_map = \"auto\", torch_dtype = torch.bfloat16)\n",
    "# Merge the LoRA layers with the base model\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save fine-tuned model at a new location\n",
    "output_merged_dir = \"../Data/Models/\"\n",
    "os.makedirs(output_merged_dir, exist_ok = True)\n",
    "model.save_pretrained(output_merged_dir, safe_serialization = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir Data/runs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResumeExtract",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
