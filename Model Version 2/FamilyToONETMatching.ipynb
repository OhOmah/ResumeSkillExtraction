{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PURPOSE OF THIS NOTEBOOK \n",
    "\n",
    "This notebook takes the data needed for the individual 22 models and trains all the models to predict which job the titles belong to once the first model predicts the job family they belong to. There will be specific challenges I will need to look through, one of these specifically being that I need to figure how to benchmark these models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FrostyBB\\AppData\\Local\\Temp\\ipykernel_4172\\2196876261.py:3: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from pathlib import Path\n",
    "from torch import cuda\n",
    "import torch\n",
    "\n",
    "from CommonFunctions import CustomDataset, BERTClass, loss_fn\n",
    "from sklearn import metrics \n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up for GPU \n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parmeters of the models \n",
    "MAX_LEN = 175\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models \n",
    "\n",
    "Just like generating the data, I would need to create a for loop that will call and train the models based on what I need to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the training/testing data\n",
    "test_df = pd.read_csv(\"../Data/MajorGroupTrainTestSplit/ONET_11_test_df.csv\")\n",
    "train_df = pd.read_csv(\"../Data/MajorGroupTrainTestSplit/ONET_11_train_df.csv\")\n",
    "label_df = pd.read_csv(\"../Data/label_df.csv\")\n",
    "onet_group_df = pd.read_csv(\"../Data/ONET_Group_list.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "onet_group_list = onet_group_df['# ONET_Group'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that the label column is actual lists instead of strings of lists\n",
    "train_df['Label'] = train_df['Label'].apply(lambda s: [float(x.strip(' []')) for x in s.split(',')])\n",
    "test_df['Label'] = test_df['Label'].apply(lambda s: [float(x.strip(' []')) for x in s.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of the data\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0}\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "               'shuffle': True,\n",
    "               'num_workers': 0}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the loop to train the data in bulk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, training_loader, optimizer):\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss= loss_fn(outputs, targets)\n",
    "        if _%100==0:\n",
    "            print(f'Epoch : {epoch}, Loss: {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, model, testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _,data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the for loop\n",
    "for group in onet_group_list:\n",
    "    # Import the data\n",
    "    train_df = pd.read_csv(f\"../Data/MajorGroupTrainTestSplit/ONET_{group}_train_df.csv\")\n",
    "    test_df = pd.read_csv(f\"../Data/MajorGroupTrainTestSplit/ONET_{group}_test_df.csv\")\n",
    "\n",
    "    # Ensuring that the label columns contain lists not strings of lists\n",
    "    train_df['Label'] = train_df['Label'].apply(lambda s: [float(x.strip(' []')) for x in s.split(',')])\n",
    "    test_df['Label'] = test_df['Label'].apply(lambda s: [float(x.strip(' []')) for x in s.split(',')])\n",
    "\n",
    "    # transform the training and testing data into the datasets needed for training \n",
    "    train_set = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
    "    test_set = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "    # Run the DataLoader on the train/test set pair\n",
    "    training_loader = DataLoader(train_set, **train_params)\n",
    "    testing_loader = DataLoader(test_set, **test_params)\n",
    "\n",
    "    # load in the model\n",
    "    model = BERTClass(len=train_df.Label.str.len()[0])\n",
    "    model.to(device)\n",
    "\n",
    "    # load in the optimizer\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        output = train(epoch, model, training_loader, optimizer)\n",
    "        outputs, targets = validation(epoch, model, testing_loader)\n",
    "        outputs = np.array(outputs) >= 0.5\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "        f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "        print(f\"Accuracy Score = {accuracy}\")\n",
    "        print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "        print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "        \n",
    "\n",
    "\n",
    "    # Lastly save the model for later testing. \n",
    "    torch.save(model, f'../Data/Models/ONET_Group_{group}_Model')\n",
    "\n",
    "    # print the model number to know which model was trained \n",
    "    print(f\"Model: {group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input, tokenizer, model, device):\n",
    "    '''\n",
    "    GOAL:\n",
    "    To take in an input, run the first model to predict the family that the occupation belongs too and then predict the occupation\n",
    "    based on the family prediction\n",
    "    '''\n",
    "    # maniputlate the output to be ready for the model \n",
    "    input = \" \".join(input.split())\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        input,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=175,\n",
    "        pad_to_max_length=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "\n",
    "    ids = torch.tensor(inputs['input_ids'], dtype=torch.long).to(device, dtype=torch.long)\n",
    "    mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).to(device, dtype=torch.long)\n",
    "    token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long).to(device, dtype=torch.long)\n",
    "\n",
    "    # Import the model family model\n",
    "    family_model = torch.load('../Data/Models/ONET_Family_Model')\n",
    "    family_model.eval()\n",
    "\n",
    "    # Run through the original model to guess family\n",
    "    onet_family = family_model(ids.unsqueeze(0), mask.unsqueeze(0), token_type_ids.unsqueeze(0))\n",
    "\n",
    "    # Extract prediction\n",
    "    onet_family_df = pd.DataFrame(onet_family.cpu().detach().numpy().tolist(), columns=onet_group_list)\n",
    "    family = onet_family_df.idmax(axis='columns')\n",
    "\n",
    "    # Load in the specific model for the family that was trained. \n",
    "    job_predict_model = torch.load(f'../Data/Models/ONET_Group_{family}_Model')\n",
    "\n",
    "    # Predict the job\n",
    "    onet_job = job_predict_model(ids.unsqueeze(0), mask.unsqueeze(0), token_type_ids.unsqueeze(0))\n",
    "\n",
    "    # Extract Prediction \n",
    "     # TODO: Get a dataframe of lists of each of the family groups and parse through it so I can extract the lists of onets families\n",
    "\n",
    "    onet_job_df = pd.DataFrame(onet_family.cpu().detach().numpy().tolist(), columns=# Get the predicted list here)\n",
    "    )\n",
    "    onet = onet_job_df.idmax(axis='columns')\n",
    "\n",
    "    return onet\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input, tokenizer, model, device):\n",
    "    ''' GOAL OF THIS FUNCTION: \n",
    "    This function takes in any given string and converts it into a tokenized version that can be run through the model. '''\n",
    "\n",
    "    input = \" \".join(input.split())\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        input,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=175,\n",
    "        pad_to_max_length=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    \n",
    "    ids = torch.tensor(inputs['input_ids'], dtype=torch.long).to(device, dtype = torch.long)\n",
    "    mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).to(device, dtype= torch.long)\n",
    "    token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long).to(device, dtype= torch.long)\n",
    "\n",
    "    output = model(ids.unsqueeze(0), mask.unsqueeze(0), token_type_ids.unsqueeze(0))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FrostyBB\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input='Data Engineer'\n",
    "output = predict(input, tokenizer, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(output.cpu().detach().numpy().tolist(), columns=onet_group_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>17</th>\n",
       "      <th>19</th>\n",
       "      <th>21</th>\n",
       "      <th>23</th>\n",
       "      <th>25</th>\n",
       "      <th>27</th>\n",
       "      <th>29</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>37</th>\n",
       "      <th>39</th>\n",
       "      <th>41</th>\n",
       "      <th>43</th>\n",
       "      <th>45</th>\n",
       "      <th>47</th>\n",
       "      <th>49</th>\n",
       "      <th>51</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-8.984849</td>\n",
       "      <td>-8.615266</td>\n",
       "      <td>3.191875</td>\n",
       "      <td>-3.806264</td>\n",
       "      <td>-7.08758</td>\n",
       "      <td>-10.594087</td>\n",
       "      <td>-11.180312</td>\n",
       "      <td>-8.736774</td>\n",
       "      <td>-6.01014</td>\n",
       "      <td>-6.921124</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.444884</td>\n",
       "      <td>-9.248122</td>\n",
       "      <td>-10.716799</td>\n",
       "      <td>-8.330391</td>\n",
       "      <td>-7.601893</td>\n",
       "      <td>-9.196458</td>\n",
       "      <td>-8.191805</td>\n",
       "      <td>-7.800813</td>\n",
       "      <td>-4.951875</td>\n",
       "      <td>-8.034726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         11        13        15        17       19         21         23  \\\n",
       "0 -8.984849 -8.615266  3.191875 -3.806264 -7.08758 -10.594087 -11.180312   \n",
       "\n",
       "         25       27        29  ...         35        37         39        41  \\\n",
       "0 -8.736774 -6.01014 -6.921124  ... -10.444884 -9.248122 -10.716799 -8.330391   \n",
       "\n",
       "         43        45        47        49        51        53  \n",
       "0 -7.601893 -9.196458 -8.191805 -7.800813 -4.951875 -8.034726  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
